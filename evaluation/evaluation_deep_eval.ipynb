{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from deepeval import evaluate\n",
        "from deepeval.metrics import GEval, FaithfulnessMetric, ContextualRelevancyMetric\n",
        "from deepeval.test_case import LLMTestCase, LLMTestCaseParams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test Correctness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added project root to path: /Users/ruhwang/Desktop/AI/my_projects/context-engineering/advanced-rag\n",
            "✅ Package imported successfully!\n",
            "LANGCHAIN_API_KEY not set (empty in .env file)\n",
            "Environment setup complete!\n",
            "=== API Keys from config.py ===\n",
            "  GROQ_API_KEY: Loaded\n",
            "  COHERE_API_KEY: Loaded\n",
            "  OPENAI_API_KEY: Loaded\n",
            "  LANGCHAIN_API_KEY: Missing\n",
            "\n",
            "=== Environment Variables ===\n",
            "  os.environ['GROQ_API_KEY']: Set\n",
            "  os.environ['COHERE_API_KEY']: Set\n",
            "\n",
            "All essential keys loaded!\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# 1. Define the directory *containing* the all_rag_techniques package\n",
        "# Get the directory of the current notebook/script (__file__ might not work in some notebooks)\n",
        "# Assuming the notebook is inside all_rag_techniques/\n",
        "current_dir = Path.cwd() \n",
        "\n",
        "# The directory containing 'all_rag_techniques' is the parent directory\n",
        "project_root = current_dir.parent \n",
        "\n",
        "# 2. Add this root to the system path\n",
        "if str(project_root) not in sys.path:\n",
        "    sys.path.insert(0, str(project_root))\n",
        "    print(f\"Added project root to path: {project_root}\")\n",
        "else:\n",
        "    print(\"Project root already in path.\")\n",
        "\n",
        "# 3. Now the import should work\n",
        "try:\n",
        "    from all_rag_techniques import setup_environment, check_keys\n",
        "    print(\"✅ Package imported successfully!\")\n",
        "    setup_environment()\n",
        "    check_keys()\n",
        "except Exception as e:\n",
        "    print(f\"❌ Final import failed: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install ipywidgets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "enum.EnumType"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(LLMTestCaseParams)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<LLMTestCaseParams.EXPECTED_OUTPUT: 'expected_output'>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "LLMTestCaseParams.EXPECTED_OUTPUT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1d3bd031b34c4cb4ac5366468973e223",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Output()"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.2916029953786671\n"
          ]
        }
      ],
      "source": [
        "correctness_metric = GEval(\n",
        "    name=\"Correctness\",\n",
        "    model=\"gpt-4o-mini\",\n",
        "    evaluation_params=[\n",
        "        LLMTestCaseParams.EXPECTED_OUTPUT,\n",
        "        LLMTestCaseParams.ACTUAL_OUTPUT],\n",
        "        evaluation_steps=[\n",
        "        \"Determine whether the actual output is factually correct based on the expected output\",\n",
        "        \"Do not heavily penalize minor semantic differences, such as capitalization or punctuation\",\n",
        "        \"if the meaning is perserved, but with different words, still give a reasonably high score\"\n",
        "    ],\n",
        ")\n",
        "\n",
        "gt_answer = \"Madrid is the capital of Spain.\"\n",
        "pred_answer = \"MadriD.\"\n",
        "\n",
        "test_case_correctness = LLMTestCase(\n",
        "    input=\"What is the capital of Spain?\",\n",
        "    expected_output=gt_answer,\n",
        "    actual_output=pred_answer,\n",
        ")\n",
        "\n",
        "correctness_metric.measure(test_case_correctness)\n",
        "print(correctness_metric.score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "276c94dd84fc444f9b75a3884610946c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Output()"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.8125496737909847\n"
          ]
        }
      ],
      "source": [
        "gt_answer = \"graph rag\"\n",
        "pred_answer = \"Graph RAG\"\n",
        "\n",
        "test_case_correctness = LLMTestCase(\n",
        "    input=\"alternative strategies to RAG for relational, knowledge-intensive tasks?\",\n",
        "    expected_output=gt_answer,\n",
        "    actual_output=pred_answer,\n",
        ")\n",
        "\n",
        "correctness_metric.measure(test_case_correctness)\n",
        "print(correctness_metric.score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test faithfulness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "deepeval.metrics.faithfulness.faithfulness.FaithfulnessMetric"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "FaithfulnessMetric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6b5256f6db984c05908b97bbe6d49c5b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Output()"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.0\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "question = \"what is 3+3?\"\n",
        "context = [\"6\"]\n",
        "generated_answer = \"6\"\n",
        "\n",
        "faithfulness_metric = FaithfulnessMetric(\n",
        "    threshold=0.7,\n",
        "    model=\"gpt-4o-mini\",\n",
        "    include_reason=False\n",
        ")\n",
        "\n",
        "test_case = LLMTestCase(\n",
        "    input = question,\n",
        "    actual_output=generated_answer,\n",
        "    retrieval_context=context\n",
        "\n",
        ")\n",
        "\n",
        "faithfulness_metric.measure(test_case)\n",
        "print(faithfulness_metric.score)\n",
        "print(faithfulness_metric.reason)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test contextual relevancy "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eb346e1e94fe4c908bff7e86652439b2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Output()"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.0\n",
            "The score is 0.00 because there are no relevant statements in the retrieval context that address the concern about whether the shoes fit, as all provided statements are unrelated.\n"
          ]
        }
      ],
      "source": [
        "actual_output = \"then go somewhere else.\"\n",
        "retrieval_context = [\"this is a test context\",\"mike is a cat\",\"if the shoes don't fit, then go somewhere else.\"]\n",
        "gpt_answer = \"if the shoes don't fit, then go somewhere else.\"\n",
        "\n",
        "relevance_metric = ContextualRelevancyMetric(\n",
        "    threshold=1,\n",
        "    model=\"gpt-4o-mini\",\n",
        "    include_reason=True\n",
        ")\n",
        "relevance_test_case = LLMTestCase(\n",
        "    input=\"What if these shoes don't fit?\",\n",
        "    actual_output=actual_output,\n",
        "    retrieval_context=retrieval_context,\n",
        "    expected_output=gpt_answer,\n",
        ")\n",
        "\n",
        "relevance_metric.measure(relevance_test_case)\n",
        "print(relevance_metric.score)\n",
        "print(relevance_metric.reason)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "new_test_case = LLMTestCase(\n",
        "    input=\"What is the capital of Spain?\",\n",
        "    expected_output=\"Madrid is the capital of Spain.\",\n",
        "    actual_output=\"MadriD.\",\n",
        "    retrieval_context=[\"Madrid is the capital of Spain.\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test two different cases together with several metrics together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Correctness </span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">[</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">GEval</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">]</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\"> Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o-mini, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span>\n",
              "<span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mCorrectness \u001b[0m\u001b[1;38;2;106;0;255m[\u001b[0m\u001b[38;2;106;0;255mGEval\u001b[0m\u001b[1;38;2;106;0;255m]\u001b[0m\u001b[38;2;106;0;255m Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o-mini, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\n",
              "\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Faithfulness Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o-mini, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mFaithfulness Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o-mini, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Contextual Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o-mini, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span>\n",
              "<span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mContextual Relevancy Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o-mini, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\n",
              "\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "29d5fd3153ea494c97ca8196f1781633",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Output()"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "LengthFinishReasonError",
          "evalue": "Could not parse response content as the length limit was reached - CompletionUsage(completion_tokens=16384, prompt_tokens=514, total_tokens=16898, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mLengthFinishReasonError\u001b[39m                   Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtest_cases\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrelevance_test_case\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_test_case\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcorrectness_metric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfaithfulness_metric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrelevance_metric\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/agentenv/lib/python3.12/site-packages/deepeval/evaluate/evaluate.py:236\u001b[39m, in \u001b[36mevaluate\u001b[39m\u001b[34m(test_cases, metrics, metric_collection, hyperparameters, identifier, async_config, display_config, cache_config, error_config)\u001b[39m\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m async_config.run_async:\n\u001b[32m    235\u001b[39m     loop = get_or_create_event_loop()\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     test_results = \u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43ma_execute_test_cases\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtest_cases\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    240\u001b[39m \u001b[43m            \u001b[49m\u001b[43midentifier\u001b[49m\u001b[43m=\u001b[49m\u001b[43midentifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    241\u001b[39m \u001b[43m            \u001b[49m\u001b[43merror_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43merror_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    242\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdisplay_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisplay_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcache_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    244\u001b[39m \u001b[43m            \u001b[49m\u001b[43masync_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43masync_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    248\u001b[39m     test_results = execute_test_cases(\n\u001b[32m    249\u001b[39m         test_cases,\n\u001b[32m    250\u001b[39m         metrics,\n\u001b[32m   (...)\u001b[39m\u001b[32m    254\u001b[39m         cache_config=cache_config,\n\u001b[32m    255\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/agentenv/lib/python3.12/site-packages/nest_asyncio.py:98\u001b[39m, in \u001b[36m_patch_loop.<locals>.run_until_complete\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f.done():\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m     97\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mEvent loop stopped before Future completed.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/agentenv/lib/python3.12/asyncio/futures.py:202\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    200\u001b[39m \u001b[38;5;28mself\u001b[39m.__log_traceback = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception.with_traceback(\u001b[38;5;28mself\u001b[39m._exception_tb)\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/agentenv/lib/python3.12/asyncio/tasks.py:316\u001b[39m, in \u001b[36mTask.__step_run_and_handle_result\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    314\u001b[39m         result = coro.send(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    315\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m         result = \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    318\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._must_cancel:\n\u001b[32m    319\u001b[39m         \u001b[38;5;66;03m# Task is cancelled right before coro stops.\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/agentenv/lib/python3.12/site-packages/deepeval/evaluate/execute.py:502\u001b[39m, in \u001b[36ma_execute_test_cases\u001b[39m\u001b[34m(test_cases, metrics, error_config, display_config, cache_config, async_config, identifier, test_run_manager, _use_bar_indicator, _is_assert_test)\u001b[39m\n\u001b[32m    499\u001b[39m                     tasks.append(asyncio.create_task(task))\n\u001b[32m    501\u001b[39m                 \u001b[38;5;28;01mawait\u001b[39;00m asyncio.sleep(async_config.throttle_value)\n\u001b[32m--> \u001b[39m\u001b[32m502\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(*tasks)\n\u001b[32m    503\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    504\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m test_case \u001b[38;5;129;01min\u001b[39;00m test_cases:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/agentenv/lib/python3.12/asyncio/tasks.py:385\u001b[39m, in \u001b[36mTask.__wakeup\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m    383\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[32m    384\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m385\u001b[39m         \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    386\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    387\u001b[39m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[32m    388\u001b[39m         \u001b[38;5;28mself\u001b[39m.__step(exc)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/agentenv/lib/python3.12/asyncio/tasks.py:316\u001b[39m, in \u001b[36mTask.__step_run_and_handle_result\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    314\u001b[39m         result = coro.send(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    315\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m         result = \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    318\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._must_cancel:\n\u001b[32m    319\u001b[39m         \u001b[38;5;66;03m# Task is cancelled right before coro stops.\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/agentenv/lib/python3.12/site-packages/deepeval/evaluate/execute.py:383\u001b[39m, in \u001b[36ma_execute_test_cases.<locals>.execute_with_semaphore\u001b[39m\u001b[34m(func, *args, **kwargs)\u001b[39m\n\u001b[32m    381\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mexecute_with_semaphore\u001b[39m(func: Callable, *args, **kwargs):\n\u001b[32m    382\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m semaphore:\n\u001b[32m--> \u001b[39m\u001b[32m383\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m func(*args, **kwargs)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/agentenv/lib/python3.12/site-packages/deepeval/evaluate/execute.py:621\u001b[39m, in \u001b[36m_a_execute_llm_test_cases\u001b[39m\u001b[34m(metrics, test_case, test_run_manager, test_results, count, test_run, ignore_errors, skip_on_missing_params, use_cache, show_indicator, _use_bar_indicator, _is_assert_test, progress, pbar_id)\u001b[39m\n\u001b[32m    619\u001b[39m new_cached_test_case: CachedTestCase = CachedTestCase()\n\u001b[32m    620\u001b[39m test_start_time = time.perf_counter()\n\u001b[32m--> \u001b[39m\u001b[32m621\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m measure_metrics_with_indicator(\n\u001b[32m    622\u001b[39m     metrics=metrics,\n\u001b[32m    623\u001b[39m     test_case=test_case,\n\u001b[32m    624\u001b[39m     cached_test_case=cached_test_case,\n\u001b[32m    625\u001b[39m     skip_on_missing_params=skip_on_missing_params,\n\u001b[32m    626\u001b[39m     ignore_errors=ignore_errors,\n\u001b[32m    627\u001b[39m     show_indicator=show_metrics_indicator,\n\u001b[32m    628\u001b[39m     pbar_eval_id=pbar_test_case_id,\n\u001b[32m    629\u001b[39m     progress=progress,\n\u001b[32m    630\u001b[39m )\n\u001b[32m    632\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m metric \u001b[38;5;129;01min\u001b[39;00m metrics:\n\u001b[32m    633\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m metric.skipped:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/agentenv/lib/python3.12/site-packages/deepeval/metrics/indicator.py:233\u001b[39m, in \u001b[36mmeasure_metrics_with_indicator\u001b[39m\u001b[34m(metrics, test_case, cached_test_case, ignore_errors, skip_on_missing_params, show_indicator, progress, pbar_eval_id, _in_component)\u001b[39m\n\u001b[32m    220\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    221\u001b[39m         tasks.append(\n\u001b[32m    222\u001b[39m             safe_a_measure(\n\u001b[32m    223\u001b[39m                 metric,\n\u001b[32m   (...)\u001b[39m\u001b[32m    230\u001b[39m             )\n\u001b[32m    231\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m233\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(*tasks)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/agentenv/lib/python3.12/asyncio/tasks.py:385\u001b[39m, in \u001b[36mTask.__wakeup\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m    383\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[32m    384\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m385\u001b[39m         \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    386\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    387\u001b[39m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[32m    388\u001b[39m         \u001b[38;5;28mself\u001b[39m.__step(exc)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/agentenv/lib/python3.12/asyncio/tasks.py:314\u001b[39m, in \u001b[36mTask.__step_run_and_handle_result\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    311\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    312\u001b[39m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[32m    313\u001b[39m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m         result = \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    315\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    316\u001b[39m         result = coro.throw(exc)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/agentenv/lib/python3.12/site-packages/deepeval/metrics/indicator.py:246\u001b[39m, in \u001b[36msafe_a_measure\u001b[39m\u001b[34m(metric, tc, ignore_errors, skip_on_missing_params, progress, pbar_eval_id, _in_component)\u001b[39m\n\u001b[32m    236\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msafe_a_measure\u001b[39m(\n\u001b[32m    237\u001b[39m     metric: Union[BaseMetric, BaseMultimodalMetric, BaseConversationalMetric],\n\u001b[32m    238\u001b[39m     tc: Union[LLMTestCase, MLLMTestCase, ConversationalTestCase],\n\u001b[32m   (...)\u001b[39m\u001b[32m    243\u001b[39m     _in_component: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    244\u001b[39m ):\n\u001b[32m    245\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m246\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m metric.a_measure(\n\u001b[32m    247\u001b[39m             tc,\n\u001b[32m    248\u001b[39m             _show_indicator=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    249\u001b[39m             _in_component=_in_component,\n\u001b[32m    250\u001b[39m             _log_metric_to_confident=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    251\u001b[39m         )\n\u001b[32m    252\u001b[39m         update_pbar(progress, pbar_eval_id)\n\u001b[32m    253\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m MissingTestCaseParamsError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/agentenv/lib/python3.12/site-packages/deepeval/metrics/faithfulness/faithfulness.py:130\u001b[39m, in \u001b[36mFaithfulnessMetric.a_measure\u001b[39m\u001b[34m(self, test_case, _show_indicator, _in_component, _log_metric_to_confident)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m metric_progress_indicator(\n\u001b[32m    121\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    122\u001b[39m     async_mode=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    123\u001b[39m     _show_indicator=_show_indicator,\n\u001b[32m    124\u001b[39m     _in_component=_in_component,\n\u001b[32m    125\u001b[39m ):\n\u001b[32m    126\u001b[39m     \u001b[38;5;28mself\u001b[39m.truths, \u001b[38;5;28mself\u001b[39m.claims = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m    127\u001b[39m         \u001b[38;5;28mself\u001b[39m._a_generate_truths(test_case.retrieval_context),\n\u001b[32m    128\u001b[39m         \u001b[38;5;28mself\u001b[39m._a_generate_claims(test_case.actual_output),\n\u001b[32m    129\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m     \u001b[38;5;28mself\u001b[39m.verdicts = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._a_generate_verdicts()\n\u001b[32m    131\u001b[39m     \u001b[38;5;28mself\u001b[39m.score = \u001b[38;5;28mself\u001b[39m._calculate_score()\n\u001b[32m    132\u001b[39m     \u001b[38;5;28mself\u001b[39m.reason = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._a_generate_reason()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/agentenv/lib/python3.12/site-packages/deepeval/metrics/faithfulness/faithfulness.py:220\u001b[39m, in \u001b[36mFaithfulnessMetric._a_generate_verdicts\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    216\u001b[39m prompt = \u001b[38;5;28mself\u001b[39m.evaluation_template.generate_verdicts(\n\u001b[32m    217\u001b[39m     claims=\u001b[38;5;28mself\u001b[39m.claims, retrieval_context=\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join(\u001b[38;5;28mself\u001b[39m.truths)\n\u001b[32m    218\u001b[39m )\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.using_native_model:\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m     res, cost = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model.a_generate(prompt, schema=Verdicts)\n\u001b[32m    221\u001b[39m     \u001b[38;5;28mself\u001b[39m.evaluation_cost += cost\n\u001b[32m    222\u001b[39m     verdicts = [item \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m res.verdicts]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/agentenv/lib/python3.12/site-packages/tenacity/asyncio/__init__.py:189\u001b[39m, in \u001b[36mAsyncRetrying.wraps.<locals>.async_wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    187\u001b[39m copy = \u001b[38;5;28mself\u001b[39m.copy()\n\u001b[32m    188\u001b[39m async_wrapped.statistics = copy.statistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m copy(fn, *args, **kwargs)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/agentenv/lib/python3.12/site-packages/tenacity/asyncio/__init__.py:111\u001b[39m, in \u001b[36mAsyncRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    109\u001b[39m retry_state = RetryCallState(retry_object=\u001b[38;5;28mself\u001b[39m, fn=fn, args=args, kwargs=kwargs)\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m     do = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter(retry_state=retry_state)\n\u001b[32m    112\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    113\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/agentenv/lib/python3.12/site-packages/tenacity/asyncio/__init__.py:153\u001b[39m, in \u001b[36mAsyncRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    151\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m action(retry_state)\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/agentenv/lib/python3.12/site-packages/tenacity/_utils.py:99\u001b[39m, in \u001b[36mwrap_to_async_func.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minner\u001b[39m(*args: typing.Any, **kwargs: typing.Any) -> typing.Any:\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/agentenv/lib/python3.12/site-packages/tenacity/__init__.py:400\u001b[39m, in \u001b[36mBaseRetrying._post_retry_check_actions.<locals>.<lambda>\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    398\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_post_retry_check_actions\u001b[39m(\u001b[38;5;28mself\u001b[39m, retry_state: \u001b[33m\"\u001b[39m\u001b[33mRetryCallState\u001b[39m\u001b[33m\"\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    399\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.iter_state.is_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.retry_run_result):\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m         \u001b[38;5;28mself\u001b[39m._add_action_func(\u001b[38;5;28;01mlambda\u001b[39;00m rs: \u001b[43mrs\u001b[49m\u001b[43m.\u001b[49m\u001b[43moutcome\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    401\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    403\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.after \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/agentenv/lib/python3.12/concurrent/futures/_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/agentenv/lib/python3.12/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/agentenv/lib/python3.12/site-packages/tenacity/asyncio/__init__.py:114\u001b[39m, in \u001b[36mAsyncRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    113\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m         result = \u001b[38;5;28;01mawait\u001b[39;00m fn(*args, **kwargs)\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[32m    116\u001b[39m         retry_state.set_exception(sys.exc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/agentenv/lib/python3.12/site-packages/deepeval/models/retry_policy.py:584\u001b[39m, in \u001b[36mcreate_retry_decorator.<locals>._decorator.<locals>.attempt\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    577\u001b[39m             logger.debug(\n\u001b[32m    578\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33masync timeout after \u001b[39m\u001b[38;5;132;01m%.3f\u001b[39;00m\u001b[33ms (active_threads=\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m, tasks=\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    579\u001b[39m                 timeout_seconds,\n\u001b[32m    580\u001b[39m                 threading.active_count(),\n\u001b[32m    581\u001b[39m                 \u001b[38;5;28mlen\u001b[39m(asyncio.all_tasks()),\n\u001b[32m    582\u001b[39m             )\n\u001b[32m    583\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m _make_timeout_error(timeout_seconds) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m584\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m coro\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/agentenv/lib/python3.12/site-packages/deepeval/models/llms/openai_model.py:354\u001b[39m, in \u001b[36mGPTModel.a_generate\u001b[39m\u001b[34m(self, prompt, schema)\u001b[39m\n\u001b[32m    352\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m schema:\n\u001b[32m    353\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model_name \u001b[38;5;129;01min\u001b[39;00m structured_outputs_models:\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m         completion = \u001b[38;5;28;01mawait\u001b[39;00m client.beta.chat.completions.parse(\n\u001b[32m    355\u001b[39m             model=\u001b[38;5;28mself\u001b[39m.model_name,\n\u001b[32m    356\u001b[39m             messages=[\n\u001b[32m    357\u001b[39m                 {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: prompt},\n\u001b[32m    358\u001b[39m             ],\n\u001b[32m    359\u001b[39m             response_format=schema,\n\u001b[32m    360\u001b[39m             temperature=\u001b[38;5;28mself\u001b[39m.temperature,\n\u001b[32m    361\u001b[39m             **\u001b[38;5;28mself\u001b[39m.generation_kwargs,\n\u001b[32m    362\u001b[39m         )\n\u001b[32m    363\u001b[39m         structured_output: BaseModel = completion.choices[\n\u001b[32m    364\u001b[39m             \u001b[32m0\u001b[39m\n\u001b[32m    365\u001b[39m         ].message.parsed\n\u001b[32m    366\u001b[39m         cost = \u001b[38;5;28mself\u001b[39m.calculate_cost(\n\u001b[32m    367\u001b[39m             completion.usage.prompt_tokens,\n\u001b[32m    368\u001b[39m             completion.usage.completion_tokens,\n\u001b[32m    369\u001b[39m         )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/agentenv/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py:1621\u001b[39m, in \u001b[36mAsyncCompletions.parse\u001b[39m\u001b[34m(self, messages, model, audio, response_format, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, safety_identifier, seed, service_tier, stop, store, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1614\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparser\u001b[39m(raw_completion: ChatCompletion) -> ParsedChatCompletion[ResponseFormatT]:\n\u001b[32m   1615\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _parse_chat_completion(\n\u001b[32m   1616\u001b[39m         response_format=response_format,\n\u001b[32m   1617\u001b[39m         chat_completion=raw_completion,\n\u001b[32m   1618\u001b[39m         input_tools=tools,\n\u001b[32m   1619\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1621\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m   1622\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m/chat/completions\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1623\u001b[39m     body=\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[32m   1624\u001b[39m         {\n\u001b[32m   1625\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: messages,\n\u001b[32m   1626\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m   1627\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33maudio\u001b[39m\u001b[33m\"\u001b[39m: audio,\n\u001b[32m   1628\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m\"\u001b[39m: frequency_penalty,\n\u001b[32m   1629\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mfunction_call\u001b[39m\u001b[33m\"\u001b[39m: function_call,\n\u001b[32m   1630\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mfunctions\u001b[39m\u001b[33m\"\u001b[39m: functions,\n\u001b[32m   1631\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mlogit_bias\u001b[39m\u001b[33m\"\u001b[39m: logit_bias,\n\u001b[32m   1632\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mlogprobs\u001b[39m\u001b[33m\"\u001b[39m: logprobs,\n\u001b[32m   1633\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmax_completion_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_completion_tokens,\n\u001b[32m   1634\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_tokens,\n\u001b[32m   1635\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m   1636\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmodalities\u001b[39m\u001b[33m\"\u001b[39m: modalities,\n\u001b[32m   1637\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m: n,\n\u001b[32m   1638\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: parallel_tool_calls,\n\u001b[32m   1639\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mprediction\u001b[39m\u001b[33m\"\u001b[39m: prediction,\n\u001b[32m   1640\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mpresence_penalty\u001b[39m\u001b[33m\"\u001b[39m: presence_penalty,\n\u001b[32m   1641\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mprompt_cache_key\u001b[39m\u001b[33m\"\u001b[39m: prompt_cache_key,\n\u001b[32m   1642\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mreasoning_effort\u001b[39m\u001b[33m\"\u001b[39m: reasoning_effort,\n\u001b[32m   1643\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mresponse_format\u001b[39m\u001b[33m\"\u001b[39m: _type_to_response_format(response_format),\n\u001b[32m   1644\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33msafety_identifier\u001b[39m\u001b[33m\"\u001b[39m: safety_identifier,\n\u001b[32m   1645\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mseed\u001b[39m\u001b[33m\"\u001b[39m: seed,\n\u001b[32m   1646\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mservice_tier\u001b[39m\u001b[33m\"\u001b[39m: service_tier,\n\u001b[32m   1647\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstore\u001b[39m\u001b[33m\"\u001b[39m: store,\n\u001b[32m   1648\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstop\u001b[39m\u001b[33m\"\u001b[39m: stop,\n\u001b[32m   1649\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   1650\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstream_options\u001b[39m\u001b[33m\"\u001b[39m: stream_options,\n\u001b[32m   1651\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: temperature,\n\u001b[32m   1652\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtool_choice\u001b[39m\u001b[33m\"\u001b[39m: tool_choice,\n\u001b[32m   1653\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m: tools,\n\u001b[32m   1654\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtop_logprobs\u001b[39m\u001b[33m\"\u001b[39m: top_logprobs,\n\u001b[32m   1655\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: top_p,\n\u001b[32m   1656\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m: user,\n\u001b[32m   1657\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mverbosity\u001b[39m\u001b[33m\"\u001b[39m: verbosity,\n\u001b[32m   1658\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mweb_search_options\u001b[39m\u001b[33m\"\u001b[39m: web_search_options,\n\u001b[32m   1659\u001b[39m         },\n\u001b[32m   1660\u001b[39m         completion_create_params.CompletionCreateParams,\n\u001b[32m   1661\u001b[39m     ),\n\u001b[32m   1662\u001b[39m     options=make_request_options(\n\u001b[32m   1663\u001b[39m         extra_headers=extra_headers,\n\u001b[32m   1664\u001b[39m         extra_query=extra_query,\n\u001b[32m   1665\u001b[39m         extra_body=extra_body,\n\u001b[32m   1666\u001b[39m         timeout=timeout,\n\u001b[32m   1667\u001b[39m         post_parser=parser,\n\u001b[32m   1668\u001b[39m     ),\n\u001b[32m   1669\u001b[39m     \u001b[38;5;66;03m# we turn the `ChatCompletion` instance into a `ParsedChatCompletion`\u001b[39;00m\n\u001b[32m   1670\u001b[39m     \u001b[38;5;66;03m# in the `parser` function above\u001b[39;00m\n\u001b[32m   1671\u001b[39m     cast_to=cast(Type[ParsedChatCompletion[ResponseFormatT]], ChatCompletion),\n\u001b[32m   1672\u001b[39m     stream=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   1673\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/agentenv/lib/python3.12/site-packages/openai/_base_client.py:1794\u001b[39m, in \u001b[36mAsyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1780\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1781\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1782\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1789\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1790\u001b[39m ) -> ResponseT | _AsyncStreamT:\n\u001b[32m   1791\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1792\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), **options\n\u001b[32m   1793\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1794\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/agentenv/lib/python3.12/site-packages/openai/_base_client.py:1599\u001b[39m, in \u001b[36mAsyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1596\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1598\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1599\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_response(\n\u001b[32m   1600\u001b[39m     cast_to=cast_to,\n\u001b[32m   1601\u001b[39m     options=options,\n\u001b[32m   1602\u001b[39m     response=response,\n\u001b[32m   1603\u001b[39m     stream=stream,\n\u001b[32m   1604\u001b[39m     stream_cls=stream_cls,\n\u001b[32m   1605\u001b[39m     retries_taken=retries_taken,\n\u001b[32m   1606\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/agentenv/lib/python3.12/site-packages/openai/_base_client.py:1688\u001b[39m, in \u001b[36mAsyncAPIClient._process_response\u001b[39m\u001b[34m(self, cast_to, options, response, stream, stream_cls, retries_taken)\u001b[39m\n\u001b[32m   1685\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(response.request.headers.get(RAW_RESPONSE_HEADER)):\n\u001b[32m   1686\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, api_response)\n\u001b[32m-> \u001b[39m\u001b[32m1688\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m api_response.parse()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/agentenv/lib/python3.12/site-packages/openai/_response.py:432\u001b[39m, in \u001b[36mAsyncAPIResponse.parse\u001b[39m\u001b[34m(self, to)\u001b[39m\n\u001b[32m    430\u001b[39m parsed = \u001b[38;5;28mself\u001b[39m._parse(to=to)\n\u001b[32m    431\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_given(\u001b[38;5;28mself\u001b[39m._options.post_parser):\n\u001b[32m--> \u001b[39m\u001b[32m432\u001b[39m     parsed = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_options\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparsed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    434\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(parsed, BaseModel):\n\u001b[32m    435\u001b[39m     add_request_id(parsed, \u001b[38;5;28mself\u001b[39m.request_id)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/agentenv/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py:1615\u001b[39m, in \u001b[36mAsyncCompletions.parse.<locals>.parser\u001b[39m\u001b[34m(raw_completion)\u001b[39m\n\u001b[32m   1614\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparser\u001b[39m(raw_completion: ChatCompletion) -> ParsedChatCompletion[ResponseFormatT]:\n\u001b[32m-> \u001b[39m\u001b[32m1615\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_parse_chat_completion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1616\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1617\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchat_completion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mraw_completion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1618\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_tools\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1619\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/agentenv/lib/python3.12/site-packages/openai/lib/_parsing/_completions.py:100\u001b[39m, in \u001b[36mparse_chat_completion\u001b[39m\u001b[34m(response_format, input_tools, chat_completion)\u001b[39m\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m choice \u001b[38;5;129;01min\u001b[39;00m chat_completion.choices:\n\u001b[32m     99\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m choice.finish_reason == \u001b[33m\"\u001b[39m\u001b[33mlength\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m LengthFinishReasonError(completion=chat_completion)\n\u001b[32m    102\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m choice.finish_reason == \u001b[33m\"\u001b[39m\u001b[33mcontent_filter\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    103\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ContentFilterFinishReasonError()\n",
            "\u001b[31mLengthFinishReasonError\u001b[39m: Could not parse response content as the length limit was reached - CompletionUsage(completion_tokens=16384, prompt_tokens=514, total_tokens=16898, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))"
          ]
        }
      ],
      "source": [
        "evaluate(\n",
        "    test_cases=[relevance_test_case, new_test_case],\n",
        "    metrics=[correctness_metric, faithfulness_metric, relevance_metric]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Function to create multiple LLMTestCases based on four lists: \n",
        "* Questions\n",
        "* Ground Truth Answers\n",
        "* Generated Answers\n",
        "* Retrieved Documents - Each element is a list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_deep_eval_test_cases(questions, gpt_answers, generated_answers, retrieved_documents):\n",
        "    return [\n",
        "        LLMTestCase(\n",
        "            input=question,\n",
        "            expected_output=gt_answer,\n",
        "            actual_output=generated_answer,\n",
        "            retrieval_context=retrieved_document\n",
        "        )\n",
        "        for question, gpt_answer, generated_answer, retrieved_document in zip(\n",
        "            questions, gpt_answers, generated_answers, retrieved_documents\n",
        "        )\n",
        "    ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](https://europe-west1-rag-techniques-views-tracker.cloudfunctions.net/rag-techniques-tracker?notebook=evaluation--evaluation-deep-eval)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "evaluation_deep_eval.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "agentenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
