{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fusion Retrieval in Document Search\n",
        "\n",
        "## Overview\n",
        "\n",
        "This code implements a Fusion Retrieval system that combines vector-based similarity search with keyword-based BM25 retrieval. The approach aims to leverage the strengths of both methods to improve the overall quality and relevance of document retrieval.\n",
        "\n",
        "## Motivation\n",
        "\n",
        "Traditional retrieval methods often rely on either semantic understanding (vector-based) or keyword matching (BM25). Each approach has its strengths and weaknesses. Fusion retrieval aims to combine these methods to create a more robust and accurate retrieval system that can handle a wider range of queries effectively.\n",
        "\n",
        "## Key Components\n",
        "\n",
        "1. PDF processing and text chunking\n",
        "2. Vector store creation using FAISS and OpenAI embeddings\n",
        "3. BM25 index creation for keyword-based retrieval\n",
        "4. Custom fusion retrieval function that combines both methods\n",
        "\n",
        "## Method Details\n",
        "\n",
        "### Document Preprocessing\n",
        "\n",
        "1. The PDF is loaded and split into chunks using RecursiveCharacterTextSplitter.\n",
        "2. Chunks are cleaned by replacing 't' with spaces (likely addressing a specific formatting issue).\n",
        "\n",
        "### Vector Store Creation\n",
        "\n",
        "1. OpenAI embeddings are used to create vector representations of the text chunks.\n",
        "2. A FAISS vector store is created from these embeddings for efficient similarity search.\n",
        "\n",
        "### BM25 Index Creation\n",
        "\n",
        "1. A BM25 index is created from the same text chunks used for the vector store.\n",
        "2. This allows for keyword-based retrieval alongside the vector-based method.\n",
        "\n",
        "### Fusion Retrieval Function\n",
        "\n",
        "The `fusion_retrieval` function is the core of this implementation:\n",
        "\n",
        "1. It takes a query and performs both vector-based and BM25-based retrieval.\n",
        "2. Scores from both methods are normalized to a common scale.\n",
        "3. A weighted combination of these scores is computed (controlled by the `alpha` parameter).\n",
        "4. Documents are ranked based on the combined scores, and the top-k results are returned.\n",
        "\n",
        "## Benefits of this Approach\n",
        "\n",
        "1. Improved Retrieval Quality: By combining semantic and keyword-based search, the system can capture both conceptual similarity and exact keyword matches.\n",
        "2. Flexibility: The `alpha` parameter allows for adjusting the balance between vector and keyword search based on specific use cases or query types.\n",
        "3. Robustness: The combined approach can handle a wider range of queries effectively, mitigating weaknesses of individual methods.\n",
        "4. Customizability: The system can be easily adapted to use different vector stores or keyword-based retrieval methods.\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "Fusion retrieval represents a powerful approach to document search that combines the strengths of semantic understanding and keyword matching. By leveraging both vector-based and BM25 retrieval methods, it offers a more comprehensive and flexible solution for information retrieval tasks. This approach has potential applications in various fields where both conceptual similarity and keyword relevance are important, such as academic research, legal document search, or general-purpose search engines."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"text-align: center;\">\n",
        "\n",
        "<img src=\"../images/fusion_retrieval.svg\" alt=\"Fusion Retrieval\" style=\"width:100%; height:auto;\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Package Installation and Imports\n",
        "\n",
        "The cell below installs all necessary packages required to run this notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added project root to path: /Users/ruhwang/Desktop/AI/my_projects/context-engineering/advanced-rag\n",
            "✅ Package imported successfully!\n",
            "LANGCHAIN_API_KEY not set (empty in .env file)\n",
            "Environment setup complete!\n",
            "=== API Keys from config.py ===\n",
            "  GROQ_API_KEY: Loaded\n",
            "  COHERE_API_KEY: Loaded\n",
            "  OPENAI_API_KEY: Loaded\n",
            "  LANGCHAIN_API_KEY: Missing\n",
            "\n",
            "=== Environment Variables ===\n",
            "  os.environ['GROQ_API_KEY']: Set\n",
            "  os.environ['COHERE_API_KEY']: Set\n",
            "\n",
            "All essential keys loaded!\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# 1. Define the directory *containing* the all_rag_techniques package\n",
        "# Get the directory of the current notebook/script (__file__ might not work in some notebooks)\n",
        "# Assuming the notebook is inside all_rag_techniques/\n",
        "current_dir = Path.cwd() \n",
        "\n",
        "# The directory containing 'all_rag_techniques' is the parent directory\n",
        "project_root = current_dir.parent \n",
        "\n",
        "# 2. Add this root to the system path\n",
        "if str(project_root) not in sys.path:\n",
        "    sys.path.insert(0, str(project_root))\n",
        "    print(f\"Added project root to path: {project_root}\")\n",
        "else:\n",
        "    print(\"Project root already in path.\")\n",
        "\n",
        "# 3. Now the import should work\n",
        "try:\n",
        "    from all_rag_techniques import setup_environment, check_keys\n",
        "    print(\"✅ Package imported successfully!\")\n",
        "    setup_environment()\n",
        "    check_keys()\n",
        "except Exception as e:\n",
        "    print(f\"❌ Final import failed: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install rank-bm25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\"\"\"\n",
        "# Clone the repository to access helper functions and evaluation modules\n",
        "!git clone https://github.com/NirDiamant/RAG_TECHNIQUES.git\n",
        "import sys\n",
        "sys.path.append('RAG_TECHNIQUES')\n",
        "# If you need to run with the latest data\n",
        "# !cp -r RAG_TECHNIQUES/data .\n",
        "\n",
        "# Load environment variables from a .env file\n",
        "load_dotenv()\n",
        "\n",
        "# Set the OpenAI API key environment variable\n",
        "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n",
        "\n",
        "# Download required data files\n",
        "import os\n",
        "os.makedirs('data', exist_ok=True)\n",
        "\n",
        "# Download the PDF document used in this notebook\n",
        "!wget -O data/Understanding_Climate_Change.pdf https://raw.githubusercontent.com/NirDiamant/RAG_TECHNIQUES/main/data/Understanding_Climate_Change.pdf\n",
        "!wget -O data/Understanding_Climate_Change.pdf https://raw.githubusercontent.com/NirDiamant/RAG_TECHNIQUES/main/data/Understanding_Climate_Change.pdf\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "from dotenv import load_dotenv\n",
        "from langchain.docstore.document import Document\n",
        "\n",
        "from typing import List\n",
        "from rank_bm25 import BM25Okapi\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Original path append replaced for Colab compatibility\n",
        "from helper_functions import *\n",
        "from evaluation.evalute_rag import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define document path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "path = \"/Users/ruhwang/Desktop/AI/my_projects/context-engineering/advanced-rag/data/Understanding_Climate_Change.pdf\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Encode the pdf to vector store and return split document from the step before to create BM25 instance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def encode_pdf_and_get_split_documents(path, chunk_size=1000, chunk_overlap=200):\n",
        "    \"\"\"\n",
        "    Encodes a PDF book into a vector store using OpenAI embeddings.\n",
        "\n",
        "    Args:\n",
        "        path: The path to the PDF file.\n",
        "        chunk_size: The desired size of each text chunk.\n",
        "        chunk_overlap: The amount of overlap between consecutive chunks.\n",
        "\n",
        "    Returns:\n",
        "        A FAISS vector store containing the encoded book content.\n",
        "    \"\"\"\n",
        "\n",
        "    # Load PDF documents\n",
        "    loader = PyPDFLoader(path)\n",
        "    documents = loader.load()\n",
        "\n",
        "    # Split documents into chunks\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size, chunk_overlap=chunk_overlap, length_function=len\n",
        "    )\n",
        "    texts = text_splitter.split_documents(documents)\n",
        "    cleaned_texts = replace_t_with_space(texts)\n",
        "\n",
        "    # Create embeddings and vector store\n",
        "    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\", dimensions=512)\n",
        "    vectorstore = FAISS.from_documents(cleaned_texts, embeddings)\n",
        "\n",
        "    return vectorstore, cleaned_texts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create vectorstore and get the chunked documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "vectorstore, cleaned_texts = encode_pdf_and_get_split_documents(path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create a bm25 index for retrieving documents by keywords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_bm25_index(documents: List[Document]) -> BM25Okapi:\n",
        "    \"\"\"\n",
        "    Create a BM25 index from the given documents.\n",
        "\n",
        "    BM25 (Best Matching 25) is a ranking function used in information retrieval.\n",
        "    It's based on the probabilistic retrieval framework and is an improvement over TF-IDF.\n",
        "\n",
        "    Args:\n",
        "    documents (List[Document]): List of documents to index.\n",
        "\n",
        "    Returns:\n",
        "    BM25Okapi: An index that can be used for BM25 scoring.\n",
        "    \"\"\"\n",
        "    # Tokenize each document by splitting on whitespace\n",
        "    # This is a simple approach and could be improved with more sophisticated tokenization\n",
        "    tokenized_docs = [doc.page_content.split() for doc in documents]\n",
        "    return BM25Okapi(tokenized_docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "bm25 = create_bm25_index(cleaned_texts) # Create BM25 index from the cleaned texts (chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<rank_bm25.BM25Okapi at 0x112e43aa0>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bm25"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "BM25 is an improvement over TF-IDF, based on probabilistic retrieval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define a function that retrieves both semantically and by keyword, normalizes the scores and gets the top k documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fusion_retrieval(vectorstore, bm25, query: str, k: int = 5, alpha: float = 0.5) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Perform fusion retrieval combining keyword-based (BM25) and vector-based search.\n",
        "\n",
        "    Args:\n",
        "    vectorstore (VectorStore): The vectorstore containing the documents.\n",
        "    bm25 (BM25Okapi): Pre-computed BM25 index.\n",
        "    query (str): The query string.\n",
        "    k (int): The number of documents to retrieve.\n",
        "    alpha (float): The weight for vector search scores (1-alpha will be the weight for BM25 scores).\n",
        "\n",
        "    Returns:\n",
        "    List[Document]: The top k documents based on the combined scores.\n",
        "    \"\"\"\n",
        "    \n",
        "    epsilon = 1e-8\n",
        "\n",
        "    # Step 1: Get all documents from the vectorstore\n",
        "    all_docs = vectorstore.similarity_search(\"\", k=vectorstore.index.ntotal)\n",
        "\n",
        "    # Step 2: Perform BM25 search\n",
        "    bm25_scores = bm25.get_scores(query.split()) # list of scores for all documents \n",
        "\n",
        "    # Step 3: Perform vector search\n",
        "    vector_results = vectorstore.similarity_search_with_score(query, k=len(all_docs)) \n",
        "    #list of (Document, score) tuples of length equal to all_docs\n",
        "    \n",
        "    # Step 4: Normalize scores\n",
        "    vector_scores = np.array([score for _, score in vector_results]) # extract scores in a numpy array\n",
        "    vector_scores = 1 - (vector_scores - np.min(vector_scores)) / (np.max(vector_scores) - np.min(vector_scores) + epsilon)\n",
        "    # normalized BM25 scores with MinMax normalization\n",
        "    bm25_scores = (bm25_scores - np.min(bm25_scores)) / (np.max(bm25_scores) -  np.min(bm25_scores) + epsilon)\n",
        "\n",
        "    # Step 5: Combine scores\n",
        "    combined_scores = alpha * vector_scores + (1 - alpha) * bm25_scores  \n",
        "\n",
        "    # Step 6: Rank documents\n",
        "    sorted_indices = np.argsort(combined_scores)[::-1] \n",
        "    # in descending order, np.argsort gives ascending order, and [::-1] reverses it\n",
        "    \n",
        "    # Step 7: Return top k documents\n",
        "    return [all_docs[i] for i in sorted_indices[:k]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Both vector_scores and bm25_scores are first normalized to the range [0, 1] using min-max scaling (higher score = more relevant).\n",
        "\n",
        "{combined_scores} is a linear weighted combination (or convex combination) of the normalized vector search scores and BM25 scores. It fuses the two retrieval methods into a single score per document, balancing their contributions based on the alpha parameter. The goal is to leverage the strengths of both: vector search (semantic similarity) and BM25 (keyword matching)\n",
        "\n",
        "1. For vectors: 1 - (score - min) / (max - min + epsilon) (inverted because vector distances are often smaller = better, but here it's adapted to make higher = better).\n",
        "\n",
        "2. For BM25: (score - min) / (max - min + epsilon) (BM25 scores are naturally higher = better).\n",
        "The formula alpha * vector_scores + (1 - alpha) * bm25_scores is a weighted average:\n",
        "It ensures the weights sum to 1 (alpha + (1 - alpha) = 1), so the combined scores stay in [0, 1].\n",
        "\n",
        "3. For each document i: combined_scores[i] = alpha * vector_scores[i] + (1 - alpha) * bm25_scores[i].\n",
        "Example: If alpha = 0.7, a document gets 70% weight from its vector score and 30% from its BM25 score.\n",
        "\n",
        "This promotes documents that perform well in both methods (reciprocal rank fusion-like, but score-based). After combination, sorting (as explained above) ranks them for the final top-k output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Use Case example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Context 1:\n",
            "challenges. This includes collaboration between scientists, policymakers, businesses, and \n",
            "communities. Interdisciplinary research and solutions are more holistic and effective. \n",
            "Citizen Science \n",
            "Citizen science involves engaging the public in scientific research and data collection. This \n",
            "empowers individuals to contribute to climate knowledge and action. Citizen science projects \n",
            "can enhance data accuracy, raise awareness, and foster community engagement. \n",
            "Hope and Inspiration \n",
            "Positive Narratives\n",
            "\n",
            "\n",
            "Context 2:\n",
            "trust and achieving policy objectives. \n",
            "International Climate Negotiations \n",
            "Conference of the Parties (COP) \n",
            "The Conference of the Parties (COP) is the supreme decision-making body of the United \n",
            "Nations Framework Convention on Climate Change (UNFCCC). COP meetings bring \n",
            "together representatives from all signatory countries to negotiate and review progress on \n",
            "climate agreements. These meetings play a crucial role in advancing global climate action. \n",
            "Climate Finance Commitments \n",
            "Developed countries have committed to providing financial support to developing countries \n",
            "to help them mitigate and adapt to climate change. Climate finance is essential for \n",
            "implementing projects, building capacity, and fostering sustainable development. Ensuring \n",
            "adequate and predictable funding is a key focus of international negotiations. \n",
            "Technology Transfer and Capacity Building \n",
            "International agreements emphasize the importance of technology transfer and capacity\n",
            "\n",
            "\n",
            "Context 3:\n",
            "water as a byproduct. Fuel cell vehicles (FCVs) offer a clean alternative to conventional \n",
            "vehicles, particularly for heavy-duty applications like trucks and buses. Developing a robust \n",
            "hydrogen infrastructure is essential for their success. \n",
            "Public Transportation Innovations \n",
            "Investments in efficient and reliable public transportation systems can reduce the number of \n",
            "private vehicles on the road, lowering emissions. Innovations include electric buses, light rail \n",
            "systems, and bike-sharing programs. Urban planning that prioritizes public transportation and \n",
            "non-motorized transit is key. \n",
            "Sustainable Agriculture and Land Use \n",
            "Precision Agriculture \n",
            "Precision agriculture uses technology to monitor and manage crop production more \n",
            "effectively. Techniques include GPS-guided equipment, soil sensors, and data analytics. \n",
            "These methods can optimize resource use, reduce emissions, and increase yields. \n",
            "Agroforestry\n",
            "\n",
            "\n",
            "Context 4:\n",
            "workshops, and community events. Lifelong learning fosters a culture of continuous \n",
            "improvement and adaptability. \n",
            "Intergenerational Dialogue \n",
            "Youth Engagement \n",
            "Engaging youth in climate action is critical for long-term sustainability. Youth bring energy, \n",
            "creativity, and a sense of urgency to climate movements. Providing platforms for youth \n",
            "voices, supporting youth-led initiatives, and involving young people in decision-making \n",
            "processes are essential for meaningful engagement. \n",
            "Intergenerational Collaboration \n",
            "Intergenerational collaboration involves working together across age groups to address \n",
            "climate challenges. This includes mentorship programs, intergenerational projects, and \n",
            "dialogue forums. Sharing knowledge and experiences between generations enhances \n",
            "collective capacity and resilience.\n",
            "\n",
            "\n",
            "Context 5:\n",
            "a long time. These projects can help sequester carbon and provide new habitats for wildlife. \n",
            "Strategic planning and ecological considerations are essential for maximizing benefits. \n",
            "Climate Policy \n",
            "Effective climate policy is essential for driving large-scale change. International agreements, \n",
            "such as the Paris Agreement, aim to limit global warming to well below 2 degrees Celsius \n",
            "above pre-industrial levels. National and local policies also play a critical role in \n",
            "implementing mitigation and adaptation strategies. \n",
            "International Agreements \n",
            "International climate agreements, such as the Kyoto Protocol and the Paris Agreement, set \n",
            "targets and frameworks for reducing greenhouse gas emissions globally. Cooperation and \n",
            "commitment from all countries are necessary for achieving climate goals. \n",
            "National Policies\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Query\n",
        "query = \"What are the impacts of climate change on the environment?\"\n",
        "\n",
        "# Perform fusion retrieval\n",
        "top_docs = fusion_retrieval(vectorstore, bm25, query, k=5, alpha=0.5)\n",
        "docs_content = [doc.page_content for doc in top_docs]\n",
        "show_context(docs_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[(Document(id='40890252-3bfd-4661-a872-a4e83d651d50', metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2024-07-13T20:17:34+03:00', 'author': 'Nir', 'moddate': '2024-07-13T20:17:34+03:00', 'source': '/Users/ruhwang/Desktop/AI/my_projects/context-engineering/advanced-rag/data/Understanding_Climate_Change.pdf', 'total_pages': 33, 'page': 12, 'page_label': '13'}, page_content='Climate change is altering terrestrial ecosystems by shifting habitat ranges, changing species \\ndistributions, and impacting ecosystem functions. Forests, grasslands, and deserts are \\nexperiencing shifts in plant and animal species composition. These changes can lead to a loss \\nof biodiversity and disrupt ecological balance. \\nMarine Ecosystems \\nMarine ecosystems are highly vulnerable to climate change. Rising sea temperatures, ocean \\nacidification, and changing currents affect marine biodiversity, from coral reefs to deep-sea \\nhabitats. Species migration and changes in reproductive cycles can disrupt marine food webs \\nand fisheries.'), \n",
        "...\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](https://europe-west1-rag-techniques-views-tracker.cloudfunctions.net/rag-techniques-tracker?notebook=all-rag-techniques--fusion-retrieval)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "agentenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
