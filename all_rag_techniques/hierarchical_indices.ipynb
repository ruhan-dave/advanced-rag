{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hierarchical Indices in Document Retrieval\n",
        "\n",
        "## Overview\n",
        "\n",
        "This code implements a Hierarchical Indexing system for document retrieval, utilizing two levels of encoding: document-level summaries and detailed chunks. This approach aims to improve the efficiency and relevance of information retrieval by first identifying relevant document sections through summaries, then drilling down to specific details within those sections.\n",
        "\n",
        "## Motivation\n",
        "\n",
        "Traditional flat indexing methods can struggle with large documents or corpus, potentially missing context or returning irrelevant information. Hierarchical indexing addresses this by creating a two-tier search system, allowing for more efficient and context-aware retrieval.\n",
        "\n",
        "## Key Components\n",
        "\n",
        "1. PDF processing and text chunking\n",
        "2. Asynchronous document summarization using OpenAI's GPT-4\n",
        "3. Vector store creation for both summaries and detailed chunks using FAISS and OpenAI embeddings\n",
        "4. Custom hierarchical retrieval function\n",
        "\n",
        "## Method Details\n",
        "\n",
        "### Document Preprocessing and Encoding\n",
        "\n",
        "1. The PDF is loaded and split into documents (likely by page).\n",
        "2. Each document is summarized asynchronously using GPT-4.\n",
        "3. The original documents are also split into smaller, detailed chunks.\n",
        "4. Two separate vector stores are created:\n",
        "   - One for document-level summaries\n",
        "   - One for detailed chunks\n",
        "\n",
        "### Asynchronous Processing and Rate Limiting\n",
        "\n",
        "1. The code uses asynchronous programming (asyncio) to improve efficiency.\n",
        "2. Implements batching and exponential backoff to handle API rate limits.\n",
        "\n",
        "### Hierarchical Retrieval\n",
        "\n",
        "The `retrieve_hierarchical` function implements the two-tier search:\n",
        "\n",
        "1. It first searches the summary vector store to identify relevant document sections.\n",
        "2. For each relevant summary, it then searches the detailed chunk vector store, filtering by the corresponding page number.\n",
        "3. This approach ensures that detailed information is retrieved only from the most relevant document sections.\n",
        "\n",
        "## Benefits of this Approach\n",
        "\n",
        "1. Improved Retrieval Efficiency: By first searching summaries, the system can quickly identify relevant document sections without processing all detailed chunks.\n",
        "2. Better Context Preservation: The hierarchical approach helps maintain the broader context of retrieved information.\n",
        "3. Scalability: This method is particularly beneficial for large documents or corpus, where flat searching might be inefficient or miss important context.\n",
        "4. Flexibility: The system allows for adjusting the number of summaries and chunks retrieved, enabling fine-tuning for different use cases.\n",
        "\n",
        "## Implementation Details\n",
        "\n",
        "1. Asynchronous Programming: Utilizes Python's asyncio for efficient I/O operations and API calls.\n",
        "2. Rate Limit Handling: Implements batching and exponential backoff to manage API rate limits effectively.\n",
        "3. Persistent Storage: Saves the generated vector stores locally to avoid unnecessary recomputation.\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "Hierarchical indexing represents a sophisticated approach to document retrieval, particularly suitable for large or complex document sets. By leveraging both high-level summaries and detailed chunks, it offers a balance between broad context understanding and specific information retrieval. This method has potential applications in various fields requiring efficient and context-aware information retrieval, such as legal document analysis, academic research, or large-scale content management systems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"text-align: center;\">\n",
        "\n",
        "<img src=\"../images/hierarchical_indices.svg\" alt=\"hierarchical_indices\" style=\"width:50%; height:auto;\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"text-align: center;\">\n",
        "\n",
        "<img src=\"../images/hierarchical_indices_example.svg\" alt=\"hierarchical_indices\" style=\"width:100%; height:auto;\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Package Installation and Imports\n",
        "\n",
        "The cell below installs all necessary packages required to run this notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install langchain langchain-openai python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone the repository to access helper functions and evaluation modules\n",
        "!git clone https://github.com/NirDiamant/RAG_TECHNIQUES.git\n",
        "import sys\n",
        "sys.path.append('RAG_TECHNIQUES')\n",
        "# If you need to run with the latest data\n",
        "# !cp -r RAG_TECHNIQUES/data ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added project root to path: /Users/ruhwang/Desktop/AI/my_projects/context-engineering/advanced-rag\n",
            "✅ Package imported successfully!\n",
            "LANGCHAIN_API_KEY not set (empty in .env file)\n",
            "Environment setup complete!\n",
            "=== API Keys from config.py ===\n",
            "  GROQ_API_KEY: Loaded\n",
            "  COHERE_API_KEY: Loaded\n",
            "  OPENAI_API_KEY: Loaded\n",
            "  LANGCHAIN_API_KEY: Missing\n",
            "\n",
            "=== Environment Variables ===\n",
            "  os.environ['GROQ_API_KEY']: Set\n",
            "  os.environ['COHERE_API_KEY']: Set\n",
            "\n",
            "All essential keys loaded!\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# 1. Define the directory *containing* the all_rag_techniques package\n",
        "# Get the directory of the current notebook/script (__file__ might not work in some notebooks)\n",
        "# Assuming the notebook is inside all_rag_techniques/\n",
        "current_dir = Path.cwd() \n",
        "\n",
        "# The directory containing 'all_rag_techniques' is the parent directory\n",
        "project_root = current_dir.parent \n",
        "\n",
        "# 2. Add this root to the system path\n",
        "if str(project_root) not in sys.path:\n",
        "    sys.path.insert(0, str(project_root))\n",
        "    print(f\"Added project root to path: {project_root}\")\n",
        "else:\n",
        "    print(\"Project root already in path.\")\n",
        "\n",
        "# 3. Now the import should work\n",
        "try:\n",
        "    from all_rag_techniques import setup_environment, check_keys\n",
        "    print(\"✅ Package imported successfully!\")\n",
        "    setup_environment()\n",
        "    check_keys()\n",
        "except Exception as e:\n",
        "    print(f\"❌ Final import failed: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import asyncio\n",
        "import os\n",
        "import sys\n",
        "from dotenv import load_dotenv\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.chains.summarize.chain import load_summarize_chain\n",
        "from langchain.docstore.document import Document\n",
        "\n",
        "# Original path append replaced for Colab compatibility\n",
        "from helper_functions import *\n",
        "from evaluation.evalute_rag import *\n",
        "from helper_functions import encode_pdf, encode_from_string"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define document path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "# Download required data files\n",
        "import os\n",
        "os.makedirs('data', exist_ok=True)\n",
        "\n",
        "# Download the PDF document used in this notebook\n",
        "!wget -O data/Understanding_Climate_Change.pdf https://raw.githubusercontent.com/NirDiamant/RAG_TECHNIQUES/main/data/Understanding_Climate_Change.pdf\n",
        "!wget -O data/Understanding_Climate_Change.pdf https://raw.githubusercontent.com/NirDiamant/RAG_TECHNIQUES/main/data/Understanding_Climate_Change.pdf\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "path = \"/Users/ruhwang/Desktop/AI/my_projects/context-engineering/advanced-rag/data/Understanding_Climate_Change.pdf\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Function to encode to both summary and chunk levels, sharing the page metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "async def encode_pdf_hierarchical(path, chunk_size=1000, chunk_overlap=200, is_string=False):\n",
        "    \"\"\"\n",
        "    Asynchronously encodes a PDF book into a hierarchical vector store using OpenAI embeddings.\n",
        "    Includes API rate limit handling with exponential backoff.\n",
        "    \n",
        "    Args:\n",
        "        path: The path to the PDF file.\n",
        "        chunk_size: The desired size of each text chunk.\n",
        "        chunk_overlap: The amount of overlap between consecutive chunks.\n",
        "        \n",
        "    Returns:\n",
        "        A tuple containing two FAISS vector stores:\n",
        "        1. Document-level summaries\n",
        "        2. Detailed chunks\n",
        "    \"\"\"\n",
        "    \n",
        "    # Load PDF documents\n",
        "    if not is_string:\n",
        "        loader = PyPDFLoader(path)\n",
        "        # This creates the original raw documents from the PDF:\n",
        "        documents = await asyncio.to_thread(loader.load)\n",
        "    else:\n",
        "        text_splitter = RecursiveCharacterTextSplitter(\n",
        "            # Set a really small chunk size, just to show.\n",
        "            chunk_size=chunk_size,\n",
        "            chunk_overlap=chunk_overlap,\n",
        "            length_function=len,\n",
        "            is_separator_regex=False,\n",
        "        )\n",
        "        documents = text_splitter.create_documents([path])\n",
        "\n",
        "    # Create document-level summaries\n",
        "    summary_llm = ChatOpenAI(temperature=0.3, model_name=\"gpt-4o-mini\", max_tokens=4000)\n",
        "    summary_chain = load_summarize_chain(summary_llm, chain_type=\"map_reduce\")\n",
        "    \n",
        "    async def summarize_doc(doc):\n",
        "        \"\"\"\n",
        "        Summarizes a single document with rate limit handling.\n",
        "        \n",
        "        Args:\n",
        "            doc: The document to be summarized.\n",
        "            \n",
        "        Returns:\n",
        "            A summarized Document object.\n",
        "        \"\"\"\n",
        "        # Retry the summarization with exponential backoff\n",
        "        summary_output = await retry_with_exponential_backoff(summary_chain.ainvoke([doc]))\n",
        "        summary = summary_output['output_text']\n",
        "        # Create a new Document for the summary, with metadata showing the source, page, and that it's a summary\n",
        "        return Document(\n",
        "            page_content=summary,\n",
        "            metadata={\"source\": path, \"page\": doc.metadata[\"page\"], \"summary\": True}\n",
        "        )\n",
        "        # The metadata in the summaries only comes from the original document that was summarized,\n",
        "\n",
        "    # Process documents in smaller batches to avoid rate limits\n",
        "    batch_size = 5  # Adjust this based on your rate limits\n",
        "    summaries = []\n",
        "    for i in range(0, len(documents), batch_size):\n",
        "        batch = documents[i:i+batch_size] # e.g., [doc0, doc1, doc2, doc3, doc4]\n",
        "        # asyncio.gather() for concurrent processing\n",
        "        batch_summaries = await asyncio.gather(*[summarize_doc(doc) for doc in batch])\n",
        "        summaries.extend(batch_summaries)\n",
        "        await asyncio.sleep(1)  # Short pause between batches\n",
        "\n",
        "    # Split documents into detailed chunks\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size, chunk_overlap=chunk_overlap, length_function=len\n",
        "    )\n",
        "    detailed_chunks = await asyncio.to_thread(text_splitter.split_documents, documents)\n",
        "\n",
        "    # Update metadata for detailed chunks: each chunk inherits metadata from its parent page, \n",
        "    # but gets additional chunk-specific info\n",
        "    for i, chunk in enumerate(detailed_chunks):\n",
        "        chunk.metadata.update({\n",
        "            \"chunk_id\": i,\n",
        "            \"summary\": False,\n",
        "            \"page\": int(chunk.metadata.get(\"page\", 0))\n",
        "        })\n",
        "\n",
        "    # Create embeddings\n",
        "    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\", dimensions=512)\n",
        "\n",
        "    # Create vector stores asynchronously with rate limit handling\n",
        "    async def create_vectorstore(docs):\n",
        "        \"\"\"\n",
        "        Creates a vector store from a list of documents with rate limit handling.\n",
        "        \n",
        "        Args:\n",
        "            docs: The list of documents to be embedded.\n",
        "            \n",
        "        Returns:\n",
        "            A FAISS vector store containing the embedded documents.\n",
        "        \"\"\"\n",
        "        return await retry_with_exponential_backoff(\n",
        "            asyncio.to_thread(FAISS.from_documents, docs, embeddings)\n",
        "        )\n",
        "        # What FAISS.from_documents() does: \n",
        "        # 1. Embeds each document: Converts page_content to a vector using OpenAI embeddings\n",
        "        # 2. Stores vectors + metadata: Each vector is paired with its metadata\n",
        "        # 3. Builds index: Creates a searchable FAISS index\n",
        "\n",
        "    # Generate vector stores for summaries and detailed chunks concurrently\n",
        "    summary_vectorstore, detailed_vectorstore = await asyncio.gather(\n",
        "        create_vectorstore(summaries),\n",
        "        create_vectorstore(detailed_chunks)\n",
        "    )\n",
        "\n",
        "    return summary_vectorstore, detailed_vectorstore"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PDF File\n",
        "    ↓ (PyPDFLoader)\n",
        "Original Documents (1 per page)\n",
        "    ↓ (split into two paths)\n",
        "    \n",
        "Path 1: Summarization    Path 2: Chunking\n",
        "    ↓                        ↓\n",
        "GPT-4o-mini summaries   RecursiveCharacterTextSplitter\n",
        "(1 summary per page)     (many chunks per page)\n",
        "    ↓                        ↓\n",
        "Add metadata:            Add metadata:\n",
        "{\"summary\": True}        {\"summary\": False, \"chunk_id\": X}\n",
        "    ↓                        ↓\n",
        "[summaries list]        [detailed_chunks list]\n",
        "    ↓                        ↓\n",
        "    ↓ asyncio.gather() → Concurrent embedding\n",
        "    ↓                        ↓\n",
        "summary_vectorstore ← FAISS index → detailed_vectorstore\n",
        "(High-level search)       (Detailed search)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Encode the PDF book to both document-level summaries and detailed chunks if the vector stores do not exist\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/Users/ruhwang/Desktop/AI/my_projects/context-engineering/advanced-rag/all_rag_techniques'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if os.path.exists(\"../vector_stores/summary_store\") and os.path.exists(\"../vector_stores/detailed_store\"):\n",
        "   embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\", dimensions=512)\n",
        "   summary_store = FAISS.load_local(\"../vector_stores/summary_store\", embeddings, allow_dangerous_deserialization=True)\n",
        "   detailed_store = FAISS.load_local(\"../vector_stores/detailed_store\", embeddings, allow_dangerous_deserialization=True)\n",
        "\n",
        "else:\n",
        "    summary_store, detailed_store = await encode_pdf_hierarchical(path)\n",
        "    summary_store.save_local(\"../vector_stores/summary_store\")\n",
        "    detailed_store.save_local(\"../vector_stores/detailed_store\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Retrieve information according to summary level, and then retrieve information from the chunk level vector store and filter according to the summary level pages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def retrieve_hierarchical(query, summary_vectorstore, detailed_vectorstore, \n",
        "                          k_summaries=3, k_chunks=5):\n",
        "    \"\"\"\n",
        "    Performs a hierarchical retrieval using the query.\n",
        "\n",
        "    Args:\n",
        "        query: The search query.\n",
        "        summary_vectorstore: The vector store containing document summaries.\n",
        "        detailed_vectorstore: The vector store containing detailed chunks.\n",
        "        k_summaries: The number of top summaries to retrieve.\n",
        "        k_chunks: The number of detailed chunks to retrieve per summary.\n",
        "\n",
        "    Returns:\n",
        "        A list of relevant detailed chunks.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Retrieve top summaries to find which pages are relevant\n",
        "    top_summaries = summary_vectorstore.similarity_search(query, k=k_summaries)\n",
        "    \n",
        "    relevant_chunks = []\n",
        "    for summary in top_summaries:\n",
        "        # For each summary, retrieve relevant detailed chunks (metadata was created in the summary steps)\n",
        "        page_number = summary.metadata[\"page\"]\n",
        "        # For each summary, retrieve relevant detailed chunks\n",
        "        page_filter = lambda metadata: metadata[\"page\"] == page_number\n",
        "        # get the relevant chunks for this page\n",
        "        page_chunks = detailed_vectorstore.similarity_search(\n",
        "            query, \n",
        "            k=k_chunks, \n",
        "            filter=page_filter\n",
        "        )\n",
        "        relevant_chunks.extend(page_chunks)\n",
        "    \n",
        "    return relevant_chunks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Demonstrate on a use case"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Page: 0\n",
            "Content: Chapter 2: Causes of Climate Change \n",
            "Greenhouse Gases \n",
            "The primary cause of recent climate change is the increase in greenhouse gases in the \n",
            "atmosphere. Greenhouse gases, such as carbon dioxide (CO2), methane (CH4), and nitrous \n",
            "oxide (N2O), trap heat from the sun, creating a \"greenhouse effect.\" This effect is essential \n",
            "for life on Earth, as it keeps the planet warm enough to support life. However, human \n",
            "activities have intensified this natural process, leading to a warmer climate. \n",
            "Fossil Fuels \n",
            "Burning fossil fuels for energy releases large amounts of CO2. This includes coal, oil, and \n",
            "natural gas used for electricity, heating, and transportation. The industrial revolution marked \n",
            "the beginning of a significant increase in fossil fuel consumption, which continues to rise \n",
            "today. \n",
            "Coal...\n",
            "---\n",
            "Page: 0\n",
            "Content: Most of these climate changes are attributed to very small variations in Earth's orbit that \n",
            "change the amount of solar energy our planet receives. During the Holocene epoch, which \n",
            "began at the end of the last ice age, human societies flourished, but the industrial era has seen \n",
            "unprecedented changes. \n",
            "Modern Observations \n",
            "Modern scientific observations indicate a rapid increase in global temperatures, sea levels, \n",
            "and extreme weather events. The Intergovernmental Panel on Climate Change (IPCC) has \n",
            "documented these changes extensively. Ice core samples, tree rings, and ocean sediments \n",
            "provide a historical record that scientists use to understand past climate conditions and \n",
            "predict future trends. The evidence overwhelmingly shows that recent changes are primarily \n",
            "driven by human activities, particularly the emission of greenhouse gases. \n",
            "Chapter 2: Causes of Climate Change \n",
            "Greenhouse Gases \n",
            "The primary cause of recent climate change is the increase in greenhouse gases in the...\n",
            "---\n",
            "Page: 0\n",
            "Content: Understanding Climate Change \n",
            "Chapter 1: Introduction to Climate Change \n",
            "Climate change refers to significant, long-term changes in the global climate. The term \n",
            "\"global climate\" encompasses the planet's overall weather patterns, including temperature, \n",
            "precipitation, and wind patterns, over an extended period. Over the past century, human \n",
            "activities, particularly the burning of fossil fuels and deforestation, have significantly \n",
            "contributed to climate change. \n",
            "Historical Context \n",
            "The Earth's climate has changed throughout history. Over the past 650,000 years, there have \n",
            "been seven cycles of glacial advance and retreat, with the abrupt end of the last ice age about \n",
            "11,700 years ago marking the beginning of the modern climate era and human civilization. \n",
            "Most of these climate changes are attributed to very small variations in Earth's orbit that \n",
            "change the amount of solar energy our planet receives. During the Holocene epoch, which...\n",
            "---\n",
            "Page: 6\n",
            "Content: protect ecosystems. Practices such as agroforestry, precision farming, and regenerative \n",
            "agriculture offer pathways to a more sustainable and resilient food system. \n",
            "By understanding the causes, effects, and potential solutions to climate change, we can take \n",
            "informed actions to protect our planet for future generations. Global cooperation, innovation, \n",
            "and commitment are key to addressing this pressing challenge. \n",
            " \n",
            "Chapter 5: The Role of Technology in Climate Change \n",
            "Mitigation \n",
            "Advanced Renewable Energy Solutions \n",
            "Next-Generation Solar Technologies...\n",
            "---\n",
            "Page: 3\n",
            "Content: The Arctic is warming at more than twice the global average rate, leading to significant ice \n",
            "loss. Antarctic ice sheets are also losing mass, contributing to sea level rise. This melting \n",
            "affects global ocean currents and weather patterns. \n",
            "Glacial Retreat \n",
            "Glaciers around the world are retreating, affecting water supplies for millions of people. \n",
            "Regions dependent on glacial meltwater, such as the Himalayas and the Andes, face \n",
            "particular risks. Glacial melt also impacts hydropower generation and agriculture. \n",
            "Coastal Erosion \n",
            "Rising sea levels and increased storm surges are accelerating coastal erosion, threatening \n",
            "homes, infrastructure, and ecosystems. Low-lying islands and coastal regions are especially \n",
            "vulnerable. Coastal communities must invest in adaptation measures like sea walls and \n",
            "managed retreats. \n",
            "Extreme Weather Events \n",
            "Climate change is linked to an increase in the frequency and severity of extreme weather...\n",
            "---\n"
          ]
        }
      ],
      "source": [
        "query = \"What is the greenhouse effect?\"\n",
        "results = retrieve_hierarchical(query, summary_store, detailed_store)\n",
        "\n",
        "# Print results\n",
        "for chunk in results:\n",
        "    print(f\"Page: {chunk.metadata['page']}\")\n",
        "    print(f\"Content: {chunk.page_content}...\")  # Print first 100 characters\n",
        "    print(\"---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](https://europe-west1-rag-techniques-views-tracker.cloudfunctions.net/rag-techniques-tracker?notebook=all-rag-techniques--hierarchical-indices)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "agentenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
