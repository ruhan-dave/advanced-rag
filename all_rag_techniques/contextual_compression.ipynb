{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Contextual Compression in Document Retrieval\n",
        "\n",
        "## Overview\n",
        "\n",
        "This code demonstrates the implementation of contextual compression in a document retrieval system using LangChain and OpenAI's language models. The technique aims to improve the relevance and conciseness of retrieved information by compressing and extracting the most pertinent parts of documents in the context of a given query.\n",
        "\n",
        "## Motivation\n",
        "\n",
        "Traditional document retrieval systems often return entire chunks or documents, which may contain irrelevant information. Contextual compression addresses this by intelligently extracting and compressing only the most relevant parts of retrieved documents, leading to more focused and efficient information retrieval.\n",
        "\n",
        "## Key Components\n",
        "\n",
        "1. Vector store creation from a PDF document\n",
        "2. Base retriever setup\n",
        "3. LLM-based contextual compressor\n",
        "4. Contextual compression retriever\n",
        "5. Question-answering chain integrating the compressed retriever\n",
        "\n",
        "## Method Details\n",
        "\n",
        "### Document Preprocessing and Vector Store Creation\n",
        "\n",
        "1. The PDF is processed and encoded into a vector store using a custom `encode_pdf` function.\n",
        "\n",
        "### Retriever and Compressor Setup\n",
        "\n",
        "1. A base retriever is created from the vector store.\n",
        "2. An LLM-based contextual compressor (LLMChainExtractor) is initialized using OpenAI's GPT-4 model.\n",
        "\n",
        "### Contextual Compression Retriever\n",
        "\n",
        "1. The base retriever and compressor are combined into a ContextualCompressionRetriever.\n",
        "2. This retriever first fetches documents using the base retriever, then applies the compressor to extract the most relevant information.\n",
        "\n",
        "### Question-Answering Chain\n",
        "\n",
        "1. A RetrievalQA chain is created, integrating the compression retriever.\n",
        "2. This chain uses the compressed and extracted information to generate answers to queries.\n",
        "\n",
        "## Benefits of this Approach\n",
        "\n",
        "1. Improved relevance: The system returns only the most pertinent information to the query.\n",
        "2. Increased efficiency: By compressing and extracting relevant parts, it reduces the amount of text the LLM needs to process.\n",
        "3. Enhanced context understanding: The LLM-based compressor can understand the context of the query and extract information accordingly.\n",
        "4. Flexibility: The system can be easily adapted to different types of documents and queries.\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "Contextual compression in document retrieval offers a powerful way to enhance the quality and efficiency of information retrieval systems. By intelligently extracting and compressing relevant information, it provides more focused and context-aware responses to queries. This approach has potential applications in various fields requiring efficient and accurate information retrieval from large document collections."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"text-align: center;\">\n",
        "\n",
        "<img src=\"../images/contextual_compression.svg\" alt=\"contextual compression\" style=\"width:70%; height:auto;\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Package Installation and Imports\n",
        "\n",
        "The cell below installs all necessary packages required to run this notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install langchain python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone the repository to access helper functions and evaluation modules\n",
        "!git clone https://github.com/NirDiamant/RAG_TECHNIQUES.git\n",
        "import sys\n",
        "sys.path.append('RAG_TECHNIQUES')\n",
        "# If you need to run with the latest data\n",
        "# !cp -r RAG_TECHNIQUES/data ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added project root to path: /Users/ruhwang/Desktop/AI/my_projects/context-engineering/advanced-rag\n",
            "✅ Package imported successfully!\n",
            "LANGCHAIN_API_KEY not set (empty in .env file)\n",
            "Environment setup complete!\n",
            "=== API Keys from config.py ===\n",
            "  GROQ_API_KEY: Loaded\n",
            "  COHERE_API_KEY: Loaded\n",
            "  OPENAI_API_KEY: Loaded\n",
            "  LANGCHAIN_API_KEY: Missing\n",
            "\n",
            "=== Environment Variables ===\n",
            "  os.environ['GROQ_API_KEY']: Set\n",
            "  os.environ['COHERE_API_KEY']: Set\n",
            "\n",
            "All essential keys loaded!\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# 1. Define the directory *containing* the all_rag_techniques package\n",
        "# Get the directory of the current notebook/script (__file__ might not work in some notebooks)\n",
        "# Assuming the notebook is inside all_rag_techniques/\n",
        "current_dir = Path.cwd() \n",
        "\n",
        "# The directory containing 'all_rag_techniques' is the parent directory\n",
        "project_root = current_dir.parent \n",
        "\n",
        "# 2. Add this root to the system path\n",
        "if str(project_root) not in sys.path:\n",
        "    sys.path.insert(0, str(project_root))\n",
        "    print(f\"Added project root to path: {project_root}\")\n",
        "else:\n",
        "    print(\"Project root already in path.\")\n",
        "\n",
        "# 3. Now the import should work\n",
        "try:\n",
        "    from all_rag_techniques import setup_environment, check_keys\n",
        "    print(\"✅ Package imported successfully!\")\n",
        "    setup_environment()\n",
        "    check_keys()\n",
        "except Exception as e:\n",
        "    print(f\"❌ Final import failed: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/rt/0kf7v29569z7rctzmpst_dsh0000gn/T/ipykernel_6370/956389574.py:10: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
            "\n",
            "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
            "with: `from pydantic import BaseModel`\n",
            "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
            "\n",
            "  from helper_functions import *\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "from dotenv import load_dotenv\n",
        "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "\n",
        "# Original path append replaced for Colab compatibility\n",
        "from helper_functions import *\n",
        "from evaluation.evalute_rag import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define document's path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download required data files\n",
        "import os\n",
        "os.makedirs('data', exist_ok=True)\n",
        "\n",
        "# Download the PDF document used in this notebook\n",
        "!wget -O data/Understanding_Climate_Change.pdf https://raw.githubusercontent.com/NirDiamant/RAG_TECHNIQUES/main/data/Understanding_Climate_Change.pdf\n",
        "!wget -O data/Understanding_Climate_Change.pdf https://raw.githubusercontent.com/NirDiamant/RAG_TECHNIQUES/main/data/Understanding_Climate_Change.pdf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "path = \"/Users/ruhwang/Desktop/AI/my_projects/context-engineering/advanced-rag/data/Understanding_Climate_Change.pdf\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create a vector store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "vector_store = encode_pdf(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "langchain_community.vectorstores.faiss.FAISS"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(vector_store)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create a retriever + contexual compressor + combine them "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "from langchain_groq import ChatGroq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a retriever\n",
        "retriever = vector_store.as_retriever()\n",
        "\n",
        "#Create a contextual compressor\n",
        "llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini\", max_tokens=4000) # ChatGroq(model='openai/gpt-oss-20b', temperature=0)\n",
        "compressor = LLMChainExtractor.from_llm(llm)\n",
        "\n",
        "#Combine the retriever with the compressor\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor,\n",
        "    base_retriever=retriever\n",
        ")\n",
        "\n",
        "# Create a QA chain with the compressed retriever\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    retriever=compression_retriever,\n",
        "    return_source_documents=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "langchain.retrievers.document_compressors.chain_extract.LLMChainExtractor"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(LLMChainExtractor.from_llm(llm))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ContextualCompressionRetriever(base_compressor=LLMChainExtractor(llm_chain=PromptTemplate(input_variables=['context', 'question'], input_types={}, output_parser=NoOutputParser(), partial_variables={}, template='Given the following question and context, extract any part of the context *AS IS* that is relevant to answer the question. If none of the context is relevant return NO_OUTPUT. \\n\\nRemember, *DO NOT* edit the extracted parts of the context.\\n\\n> Question: {question}\\n> Context:\\n>>>\\n{context}\\n>>>\\nExtracted relevant parts:')\n",
              "| ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x169156f30>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x169157a40>, root_client=<openai.OpenAI object at 0x106876210>, root_async_client=<openai.AsyncOpenAI object at 0x169156bd0>, model_name='gpt-5-nano', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'), max_tokens=4000)\n",
              "| NoOutputParser(), get_input=<function default_get_input at 0x12b48d580>), base_retriever=VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x1691558e0>, search_kwargs={}))"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "compression_retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "RetrievalQA(verbose=False, combine_documents_chain=StuffDocumentsChain(verbose=False, llm_chain=LLMChain(verbose=False, prompt=ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template=\"Use the following pieces of context to answer the user's question. \\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n----------------\\n{context}\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})]), llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x169156f30>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x169157a40>, root_client=<openai.OpenAI object at 0x106876210>, root_async_client=<openai.AsyncOpenAI object at 0x169156bd0>, model_name='gpt-5-nano', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'), max_tokens=4000), output_parser=StrOutputParser(), llm_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_variable_name='context'), return_source_documents=True, retriever=ContextualCompressionRetriever(base_compressor=LLMChainExtractor(llm_chain=PromptTemplate(input_variables=['context', 'question'], input_types={}, output_parser=NoOutputParser(), partial_variables={}, template='Given the following question and context, extract any part of the context *AS IS* that is relevant to answer the question. If none of the context is relevant return NO_OUTPUT. \\n\\nRemember, *DO NOT* edit the extracted parts of the context.\\n\\n> Question: {question}\\n> Context:\\n>>>\\n{context}\\n>>>\\nExtracted relevant parts:')\n",
              "| ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x169156f30>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x169157a40>, root_client=<openai.OpenAI object at 0x106876210>, root_async_client=<openai.AsyncOpenAI object at 0x169156bd0>, model_name='gpt-5-nano', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'), max_tokens=4000)\n",
              "| NoOutputParser(), get_input=<function default_get_input at 0x12b48d580>), base_retriever=VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x1691558e0>, search_kwargs={})))"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "qa_chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "langchain.chains.retrieval_qa.base.RetrievalQA"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "RetrievalQA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example usage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ChatGroq (gpt-oss-20b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The document’s main topic is **climate change**—specifically its broad impacts on human health, society, and the environment, and the policies, actions, and justice considerations needed to address those challenges.\n",
            "Source documents: [Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2024-07-13T20:17:34+03:00', 'author': 'Nir', 'moddate': '2024-07-13T20:17:34+03:00', 'source': '/Users/ruhwang/Desktop/AI/my_projects/context-engineering/advanced-rag/data/Understanding_Climate_Change.pdf', 'total_pages': 33, 'page': 14, 'page_label': '15'}, page_content='Chapter 10: Climate Change and Human Health \\nHealth Impacts'), Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2024-07-13T20:17:34+03:00', 'author': 'Nir', 'moddate': '2024-07-13T20:17:34+03:00', 'source': '/Users/ruhwang/Desktop/AI/my_projects/context-engineering/advanced-rag/data/Understanding_Climate_Change.pdf', 'total_pages': 33, 'page': 9, 'page_label': '10'}, page_content='Chapter 6: Global and Local Climate Action'), Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2024-07-13T20:17:34+03:00', 'author': 'Nir', 'moddate': '2024-07-13T20:17:34+03:00', 'source': '/Users/ruhwang/Desktop/AI/my_projects/context-engineering/advanced-rag/data/Understanding_Climate_Change.pdf', 'total_pages': 33, 'page': 27, 'page_label': '28'}, page_content='Extracted relevant parts:\\nLegacy for Future Generations \\nOur actions today shape the world for future generations. Ensuring a sustainable and resilient \\nplanet is our responsibility to future generations. By working together, we can create a legacy \\nof environmental stewardship, social equity, and global solidarity. \\nChapter 19: Climate Change and Policy \\nPolicy Development and Implementation \\nNational Climate Policies \\nCountries around the world are developing and implementing national climate policies to \\naddress climate change. These policies set emission reduction targets, promote renewable \\nenergy, and support adaptation measures. Effective policy implementation requires'), Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2024-07-13T20:17:34+03:00', 'author': 'Nir', 'moddate': '2024-07-13T20:17:34+03:00', 'source': '/Users/ruhwang/Desktop/AI/my_projects/context-engineering/advanced-rag/data/Understanding_Climate_Change.pdf', 'total_pages': 33, 'page': 18, 'page_label': '19'}, page_content='This vision includes a healthy planet, thriving ecosystems, and equitable societies. Working together towards this vision creates a sense of purpose and motivation. By embracing these principles and taking concerted action, we can address the urgent challenge of climate change and build a sustainable, resilient, and equitable world for all. The path forward requires courage, commitment, and collaboration, but the rewards are immense—a thriving planet and a prosperous future for generations to come.  \\n\\nChapter 13: Climate Change and Social Justice  \\n\\nClimate Justice  \\n\\nUnderstanding Climate Justice  \\n\\nClimate justice emphasizes the ethical dimensions of climate change, recognizing that its impacts are not evenly distributed. Vulnerable populations, including low-income communities, indigenous peoples, and marginalized groups, often face the greatest risks while contributing the least to greenhouse gas emissions. Climate justice advocates for')]\n"
          ]
        }
      ],
      "source": [
        "query = \"What is the main topic of the document?\"\n",
        "result = qa_chain.invoke({\"query\": query})\n",
        "print(result[\"result\"])\n",
        "print(\"Source documents:\", result[\"source_documents\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](https://europe-west1-rag-techniques-views-tracker.cloudfunctions.net/rag-techniques-tracker?notebook=all-rag-techniques--contextual-compression)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "agentenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
